{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science – Lecture 13 – Web Scraping \n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/* \n",
    "\n",
    "In this lecture we will explore how we can extract data from a web-page using automatic scraping and crawling with [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "### To understand web scraping, we'll first need to talk a bit about HTML though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML and the DOM\n",
    "\n",
    "We will scrape web-pages that are (partially) written in HTML and represented in the DOM. \n",
    "\n",
    "*DOM* stands for  *Document Object Model*, while *HTML* stands for *“HyperText Markup Language”*. Thirsty years ago, that used to be a meaningful description of what HTML actually did: it has links (hypertext), and it is a markup language. \n",
    "\n",
    "The latest version of HTML, however, the HTML5 standard, does much, much more: graphics, audio, video, etc. So it is easier to think of HTML as “whatever it is that web browsers know how to interpret”, and just not think about the actual term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements\n",
    "\n",
    "The important thing about HTML is that the markup is represented by *elements*. An HTML element is a portion of the content that is surrounded by a pair of tags of the same name. Like this:\n",
    "\n",
    "```html\n",
    "<strong>This is an HTML element.</strong>\n",
    "```\n",
    "\n",
    "In this element, **strong** is the name of the **tag**; the *open* tag is `<strong>`, and the *matching closing tag* is `</strong>`. The way you should interpret this is that the text “This is an HTML element” should be “strong”, i.e., typically this will be bold text.\n",
    "\n",
    "HTML elements can and commonly do nest:\n",
    "\n",
    "```html\n",
    "<strong>This is strong, and <u>this is underlined and strong.</u></strong>\n",
    "```\n",
    "\n",
    "The nesting might be easier to see if we format it like this:\n",
    "```html\n",
    "<strong>\n",
    "    This is strong, and \n",
    "    <u>\n",
    "        this is underlined and strong.\n",
    "    </u>\n",
    "</strong>\n",
    "```\n",
    "\n",
    "In addition to the names, opening tags can contain extra information about the element. These are called attributes:\n",
    "\n",
    "```html\n",
    "<a href='http://www.google.com'>A link to Google's main page</a>\n",
    "```\n",
    "\n",
    "In this case, we’re using the `a` element which stood for “anchor”, but now is almost universally used as a “link”. The attribute `href` means “HTML reference”. The meaning given to each attribute changes from element to element.\n",
    "\n",
    "Other important attributes for our purposes are `id` and `class`. The **id** attribute gives the attribute a unique identifier, which can then be used to access the element programmatically. Think of it as making the element accessible via a global variable.  \n",
    "\n",
    "The **class** is similar but is intended to be applied to a whole “class” of elements. \n",
    "\n",
    "HTML pages require some boilerplate. Here is a minimal page: \n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title></title>\n",
    "</head>\n",
    "<body>\n",
    "Hello World! What's up?\n",
    "</body>\n",
    "</html>\n",
    "``` \n",
    "\n",
    "The `<head>` contains meta-information such as the title of the site, the `<body>` contains the actual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy\n",
    "\n",
    "Data in HTML is often structured hierarchically: \n",
    "\n",
    "```html\n",
    "<body>\n",
    "  <article>\n",
    "    <span class=\"date\">Published: 1969-10-22</span>\n",
    "    <span class=\"author\">Led Zeppelin</span>\n",
    "    <h1>Ramble On</h1>\n",
    "    <div class=\"content\">\n",
    "    Leaves are falling all around, It's time I was on my way. \n",
    "    Thanks to you, I'm much obliged for such a pleasant stay. \n",
    "    But now it's time for me to go. The autumn moon lights my way. \n",
    "    For now I smell the rain, and with it pain, and it's headed my way. \n",
    "    </div>\n",
    "  </article>\n",
    "  <article>\n",
    "    <span class=\"date\">Published: 2016-05-03</span>\n",
    "    <span class=\"author\">Radiohead</span>\n",
    "    <h1>Burn the Witch</h1>\n",
    "    <div class=\"content\">\n",
    "    Stay in the shadows\n",
    "    Cheer at the gallows\n",
    "    This is a round up\n",
    "    This is a low flying panic attack\n",
    "    Sing a song on the jukebox that goes\n",
    "    Burn the witch\n",
    "    Burn the witch\n",
    "    We know where you live\n",
    "    </div>\n",
    "  </article>\n",
    "</body>\n",
    "```\n",
    "\n",
    "Here, the title of the song is nested three levels deep: `body > article > h1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables\n",
    "\n",
    "Data is also often stored in HTML tables, which are enclosed in a `<table>` tag. `<tr>` indicates a row (table row), `<th>` and `<td>` are used to demark cells, either header cells (`<th>`, table header) or regular cells (`<td>`, table data). Here's an example: \n",
    "\n",
    "```html\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>The Beatles</th>\n",
    "        <th>Led Zeppelin</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td># Band Members</td>\n",
    "        <td>4</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "</table>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DOM\n",
    "\n",
    "As we have seen above, a markup document looks a lot like a tree: it has a root, the HTML element, and elements can have children that are containing elements themselves.\n",
    "\n",
    "While HTML is a textual representation of a markup document, the DOM is a programming interface for it. Also the DOM represents the state of a page as it's rendered, that (nowadays) doesn't mean that there is an underlying HTML document that corresponds to that exactly. Rather, the DOM is dynamically generated with, e.g., JavaScript. \n",
    "\n",
    "In this class we will use “DOM” to mean the tree created by the web browsers to represent the document.\n",
    "\n",
    "#### Inspecting the DOM in a browser\n",
    "\n",
    "Perhaps the most important habit when scraping is to investigate the source of a page using the Developer Tools. In this case, we’ll look at the **element tree**, by clicking on the menu bar: View → Developer → Developer Tools.\n",
    "\n",
    "Alternatively, you can right click on any part of the webpage, and choose “Inspect Element”. Notice that there can be a big difference between what is in the DOM and what is in the source.\n",
    "\n",
    "Take a look at the DOM of [this html page](lyrics.html). Next, we'll scrape the data from this page! \n",
    "\n",
    "![Example fo Dev Tools and the sample page](sampledevtools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping with BeautifulSoup\n",
    "\n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a Python library design for computationally extracting data from html documents. It supports navigating in the DOM and retrieving exactly the data elements you need.\n",
    "\n",
    "Let's start with a simple example using the [lyrics.html](lyrics.html) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we tell BeautifulSoup and tell it which parser to use\n",
    "song_soup = BeautifulSoup(open(\"lyrics.html\"), \"html.parser\")\n",
    "# the output corresponds exactly to the html file\n",
    "song_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, generated (minified) html can be hard to read (not in this case), so we can format it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(song_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access content by tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the title tag\n",
    "song_soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get the text out of the tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly accessing an element works for the first occurence of a tag, we don't get the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup.div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can retreive the text content of an element: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(song_soup.div.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use attributes to find a specific element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup.find(id=\"zep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can also get only the text, not the html markup with [`find()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = song_soup.find(id=\"zep\").get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use [`find_all()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#calling-a-tag-is-like-calling-find-all) to get all instances of a tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1s = song_soup.find_all(\"h1\")\n",
    "h1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a list of beautiful soup elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(h1s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to get the text out of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_h1s = [tag.get_text() for tag in h1s]\n",
    "string_h1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find elements by ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup.find_all(id=\"zep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by class (notice the keyword here is `class_` because the word `class` is reserved in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup.find_all(class_=\"content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `find_all` is so commonly used, you can use a shortcut by just calling directly on an object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup(\"div\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can address the elements in the returned object directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup(\"div\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or iterate over it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in song_soup.find_all(\"div\"):\n",
    "    print(\"###################\")\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS Selectors\n",
    "\n",
    "We can also use CSS selectors. CSS Selectors apply, among others, to elements, classes, and IDs.\n",
    "\n",
    "Below is an example of how CSS is used to style different elements. \n",
    "\n",
    "\n",
    "```CSS\n",
    "/* Element Selector */\n",
    "article {\n",
    "  color: FireBrick;\n",
    "}\n",
    "\n",
    "/* ID selector */\n",
    "#myID {\n",
    "  color: Tomato;\n",
    "}\n",
    "\n",
    "/* Class selector */\n",
    ".myClass {\n",
    "  color: Aquamarine;\n",
    "}\n",
    "\n",
    "/* Child selector. Only DIRECT children match */\n",
    "p > b {\n",
    "  color: SteelBlue;\n",
    "}\n",
    "\n",
    "/* Descendant selector. Every time a b is nested within a div this matches */\n",
    "div b {\n",
    "  color: green;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "[Here is an example](https://jsfiddle.net/gxhqv26m/1/) with all the important selectors.\n",
    "\n",
    "Let's try this in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting all elements of class .content\n",
    "song_soup.select(\".content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting all divs that are somewhere below the id radio in the tree\n",
    "song_soup.select(\"#radio div\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it!\n",
    "\n",
    "Get the list of all published dates from the file. \n",
    "\n",
    "1. Get all the spans with the date\n",
    "2. Get just the dates themselves (e.g., '1969-10-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we know how to extract information out of a website. Let's look at a complete example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching a Website\n",
    "\n",
    "Downloading websites is easy and very efficient. It turns out, that you can cause quite a high load on a server when you scrape it. To avoid that, webmasters usually publish what kinds of scraping they allow on their websites. You should check out a website's terms of service and the `robots.txt` of a domain before crawling excessively. Terms of service are usually broad, so searching for “scraping” or “crawling” is a good idea.\n",
    "\n",
    "Let's take a look at [Google Scholar's robots.txt](https://scholar.google.com/robots.txt):\n",
    "\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /search\n",
    "Allow: /search/about\n",
    "Disallow: /sdch\n",
    "Disallow: /groups\n",
    "Disallow: /index.html?\n",
    "Disallow: /?\n",
    "Allow: /?hl=\n",
    "...\n",
    "Disallow: /scholar\n",
    "Disallow: /citations?\n",
    "...\n",
    "```\n",
    "\n",
    "Here it specifies that you're not allowed to crawl a lot of the pages. The `/scholar` subdirectory is especially painful because it prohibits you from generating queries dynamically. \n",
    "\n",
    "It's also common that sites ask you to delay crawiling: \n",
    "\n",
    "```\n",
    "Crawl-delay: 30 \n",
    "Request-rate: 1/30 \n",
    "```\n",
    "\n",
    "You should respect those restrictions. Now, no one can stop you from running a request through a crawler, but sites like google scholar will block you VERY quickly if you request to many pages in a short time-frame. It's also common that bigger sites serve up a CAPTCHA if they think you're using a bot to crawl. \n",
    "\n",
    "An alternative strategy to dynamically accessing the site you're crawling (as we're doing in the next example) is to download a local copy of the website and crawl that. This ensures that you hit the site only once per page. A good tool to achieve that is [wget](https://www.gnu.org/software/wget/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Utah's course enrollments\n",
    "\n",
    "We're going to build a dataset of classes offered this fall at the U and look at the enrollment numbers. We'll use the catalog listed here:  \n",
    "https://class-schedule.app.utah.edu/main/1254/index.html\n",
    "\n",
    "The U doesn't seem to care whether/how we crawl the websites, the [fineprint](https://www.utah.edu/disclaimer/) doesn't mention it and neither does the `robots.txt`: http://www.utah.edu/robots.txt\n",
    "\n",
    "We'll use the [`urllib.request`](https://docs.python.org/3.0/library/urllib.request.html) library to retreive the websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"https://class-schedule.app.utah.edu/main/1254/\"\n",
    "# here we actually access the website\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    html = response.read()\n",
    "    html = html.decode('utf-8')\n",
    "\n",
    "# save the file\n",
    "with open('class_schedule.html', 'w') as new_file:\n",
    "    new_file.write(html)\n",
    "\n",
    "# here it's already a local operation\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first 5000 lines of this page: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()[0:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to get out of this page is the link to the subject specific subject course lists.\n",
    "\n",
    "While you could find this in the text output in python, it's much easier to find the relevant parts in your browser's built in inspector. Here, we've highlighted one of the subjects (Accounting):  \n",
    "\n",
    "![Chrome Inspector](inspector.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the relevant HTML snippet: \n",
    "\n",
    "```html\n",
    "<div class=\"d-grid d-md-block gap-2\">\n",
    "    <a class=\"btn btn-light m-1\" href=\"class_list.html?subject=ACCTG\">ACCTG - Accounting</a>\n",
    "</div>\n",
    "```\n",
    "\n",
    "We can look at some of the others and notice they all use the `<a>` element and the `m-1` class attribute. \n",
    "\n",
    "We thus hypothesize we can use these together to scrape.\n",
    "\n",
    "*(Note, based on this quick search, we may hypothesize we could use `btn-light` instead of `m-1`, but when we search in the dev tools inspector for `a.btn-light` we see it brings up things we don't want...)*\n",
    "\n",
    "![btn-light doesn't work](btnlight.png)\n",
    "\n",
    "\n",
    "Let's build a dictionary of subject shorthands to a tuple of the full subject name and links to the relevant courses. For CS this should look like this: \n",
    "\n",
    "```\n",
    "CS: (Computer Science, https://class-schedule.app.utah.edu/main/1254/class_list.html?subject=CS)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = {}\n",
    "\n",
    "# use the select statement with element.class syntax to retrieve the relevant subjects\n",
    "for subject in soup.select(\"a.m-1\"):\n",
    "    # get the link out of the href attribute\n",
    "    link_tail = subject.get(\"href\")\n",
    "    # concatenate the base URL and the tail of the link\n",
    "    link = url + link_tail\n",
    "    # the subject shortname is embedded within the <a> tag, we split by a dash and spaces\n",
    "    subject_text = subject.get_text().split(\" - \")\n",
    "    # write it to the dictionary\n",
    "    subjects[subject_text[0]] = (subject_text[1], link)\n",
    "\n",
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the entry for math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects[\"MATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what we want. \n",
    "\n",
    "As an aside: we could have taken a different approach here. Note how the URL has a deterministic query parameter that matches the subject:\n",
    "\n",
    "```\n",
    "class_list.html?subject=MATH\n",
    "```\n",
    "\n",
    "We could use this to also retrieve the links if we only had the subject shortnames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a list of classes\n",
    "\n",
    "Next, it's time to get the courses. Let's look at the [website for CS](https://student.apps.utah.edu/uofu/stu/ClassSchedules/main/1204/class_list.html?subject=CS).\n",
    "\n",
    "We'll fetch this class list in a function where we pass in the subject name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWebsiteAsSoup(url):\n",
    "    \"\"\" \n",
    "    Retrieve a website and return it as a BeautifulSoup object.   \n",
    "    \"\"\"\n",
    "    \n",
    "    req = urllib.request.Request(url)\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        classlist_html = response.read()\n",
    "    \n",
    "    class_soup = BeautifulSoup(classlist_html, 'html.parser')\n",
    "    with open('class_list.html', 'w') as new_file:\n",
    "        new_file.write(str(class_soup))\n",
    "        \n",
    "    return class_soup        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this function for CS and look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_soup = getWebsiteAsSoup(subjects[\"CS\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(class_soup.prettify()[0:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's analyze this html in the inspector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Class Site Inspected](class.png)\n",
    "\n",
    "\n",
    "\n",
    "The data is stored in a div that has the ID `class-details`. Here is the relevant section for an individual class: \n",
    "\n",
    "\n",
    "```html\n",
    "<h3>\n",
    "<a href=\"sections.html?subj=CS&amp;catno=1400\">CS 1400</a>\n",
    "                        -\n",
    "              <span>001</span>\n",
    "              <span>Intro Comp Programming</span>\n",
    "</h3>\n",
    "```\n",
    "\n",
    "Let's extract this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# to remove duplicates\n",
    "course_numbers = set();\n",
    "# we'll store the data here \n",
    "course_rows = []\n",
    "\n",
    "# here we get the \".card-body\" children of the element with the #class-details id; \n",
    "# this returns a list of soup objects that contain the relevant info for each class.\n",
    "course_container = class_soup.select(\"#class-details .card-body\")\n",
    "\n",
    "for course in course_container:\n",
    "    # print(course)\n",
    "    \n",
    "    # we get the link that is part of a heading 3 element and retrieve the first one\n",
    "    course_info = course.select(\"h3\")[0].get_text()\n",
    "    # print(course_info)\n",
    "    \n",
    "    # The first one has two links, duplicate courses (sections) sometimes have only one. \n",
    "    number, name = course_info.split(\" -\")[0:2]\n",
    "    # get rid of white space\n",
    "    number = number.strip()\n",
    "    # the name is of the format \"001\\nNetworking Seminar\\n'\" We get rid of the section number.\n",
    "    name = name.strip()[4:]\n",
    "    #print(number, name)\n",
    "    \n",
    "    # Avoid the duplicates - some courses are listed multiple times, we'll grab the first instance\n",
    "    # Note this might miss duplicated sections though!\n",
    "    if (number in course_numbers):\n",
    "        continue\n",
    "    course_numbers.add(number)\n",
    "\n",
    "    # get the URL \n",
    "    link = url + course.select(\"h3 a\")[0].get(\"href\")\n",
    "    \n",
    "    # the course info is contained in a unordered list. Here we get the list items. \n",
    "    info_list = course.select(\"ul li\")\n",
    "    \n",
    "    # here we get the second element that contains the instructor information, get the text, and split it by : \n",
    "    instructor = info_list[1].get_text().split(\":\")[1]\n",
    "    # more cleanup - we keep the stuff before the - and then clear up the whitespace\n",
    "    instructor = instructor.split(\"-\")[0].strip()\n",
    "           \n",
    "    # We store this as a dictionary in our course_rows lists. \n",
    "    # The dictionary is well suited to later initialize a data frame\n",
    "    course_rows.append({\"Number\":number, \"Name\":name, \"Instructor\":instructor, \"Link\":link, \"Students\":0})  \n",
    "    \n",
    "    \n",
    "# create the data frame    \n",
    "courses = pd.DataFrame(course_rows)    \n",
    "    \n",
    "# as always, check to make sure our output looks right    \n",
    "courses.head(15)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask questions, such as who's teaching the most course numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses[\"Instructor\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Enrollment\n",
    "\n",
    "Now let's look at which classes have the most students. Here's an example for the [Algorithms and Data Structures class](https://class-schedule.app.utah.edu/main/1254/sections.html?subj=CS&catno=2420).\n",
    "\n",
    "Let's scrape the number of students from this website. \n",
    "\n",
    "The data is contained in a `<table>`. When we have a table, there is an easy way to get the data out – use the magic of Pandas data import functions. There is a sophisticated [`read_html()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html) function. \n",
    "\n",
    "Behind the scenes, this uses html5lib, which should be part of your anaconda installation, but if not, you will have to install it:\n",
    "\n",
    "```\n",
    "conda install -c anaconda html5lib\n",
    "```\n",
    "\n",
    "Once installed, we can pass in the table as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollment_url = \"https://class-schedule.app.utah.edu/main/1254/sections.html?subj=CS&catno=2420\"\n",
    "with urllib.request.urlopen(enrollment_url) as response:\n",
    "    class_student_html = response.read()\n",
    "class_student_soup = BeautifulSoup(class_student_html, 'html.parser')\n",
    "\n",
    "# we use pandas to read the table, this returns an array of dataframes, we take the first table\n",
    "student_ar = pd.read_html(str(class_student_soup))[0]\n",
    "student_ar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can retrieve the student number; most classes have one \"overall\" section (section 1), and further sections for recitations. So we'll just use the first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = student_ar[\"Currently Enrolled\"][0]\n",
    "students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pack this all up in a function that takes the URL and returns the number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_students(url):\n",
    "    \"\"\"\n",
    "    Retreive the number of students from the table on the website specified in the URL\n",
    "    \"\"\"\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        class_student_html = response.read()\n",
    "    class_student_soup = BeautifulSoup(class_student_html, 'html.parser')\n",
    "    # we use pandas to read the table\n",
    "    student_df = pd.read_html(str(class_student_soup))[0]\n",
    "    students = student_df[\"Currently Enrolled\"][0]\n",
    "    return students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run this for all the courses from the data structure that we built before using the apply function. \n",
    "\n",
    "Note that this actually accesses the websites and thus takes a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses[\"Students\"] = courses[\"Link\"].apply(scrape_students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at which are the largest classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = courses.sort_values(\"Students\", ascending=False)\n",
    "courses = courses.reset_index(drop=True)\n",
    "courses.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database class is the biggest one, followed by the object-oriented programming class. Note the latter is where you really get into the concepts of Classes and Objects in programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to answer the question **at which level are the most students enrolled in CS?** Levels are the leading digit of the course number and roughly correspond to the year you're supposed to take a class. 5000 level classes are advanced undergrad classes, often cross-listed with grad classes. 6000 classes are graduate classes, 7000 classes are seminars and other PhD level classes. Let's create a new column level and add it to the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use x[3] because the strings such as \"CS 1410\" have the level digit at index 3\n",
    "courses[\"level\"] = courses[\"Number\"].map(lambda x: x[3])\n",
    "courses = courses.sort_values(by=\"level\")\n",
    "\n",
    "# Let's check to see if what we just did gave us the output we wanted\n",
    "courses.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the answer with groupby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We only apply the np functions to the numeric column\n",
    "student_stats = courses.groupby(by=\"level\").aggregate({\"Students\" : [\"count\", np.sum, np.mean, np.std]})\n",
    "\n",
    "# Check to see if this makes sense\n",
    "student_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this with box and overlaid strip plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "\n",
    "# there isn't enough data for a violin plot to work well. \n",
    "ax = sns.boxplot(data=courses, x=\"level\", y=\"Students\", fliersize=0)\n",
    "ax = sns.stripplot(data=courses, x=\"level\", y=\"Students\", color=\"black\", alpha=0.5, size=10)\n",
    "\n",
    "# props = dict(linewidth=2)\n",
    "# here we group by level\n",
    "# courses.boxplot(by='level', figsize=(10, 10),  boxprops=props, medianprops=props, whiskerprops=props, capprops=props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot a bar chart for the number of students at each level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_sum = courses.groupby(\"level\").aggregate([np.sum])\n",
    "student_sum.plot.bar(legend=False, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most students are in the 3000 level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Wrap-Up\n",
    "\n",
    "Scraping is a way to get information from website that were not designed to make data accessible. As such, it can often be **brittle**: a website change will break your scraping script. It is also often not welcome, as a scaper can cause a lot of traffic. \n",
    "\n",
    "The way we scraped information here also made the **assumption that HTML is generated consistently** based just on the URL. That is, unfortunately, less and less common, as websites adapt to browser types, resolutions, locales, but also as a lot of content is loaded dynamically e.g., via web-sockets. For example, many websites now auotmatically load more data once you scroll to the bottom of the page. These websites couldn't be scraped with our approach, instead, a browser-emulation approach, using e.g., [Selenium](https://www.selenium.dev/) would be necessary. [Here is a tutorial](https://medium.com/the-andela-way/introduction-to-web-scraping-using-selenium-7ec377a8cf72) on how to do that. \n",
    "\n",
    "Finally, many services make their data available through a well-defined interface, an API. Using an API is always a better idea than scraping, but scraping is a good fallback!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
